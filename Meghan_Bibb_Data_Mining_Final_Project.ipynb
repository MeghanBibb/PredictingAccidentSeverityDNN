{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Meghan_Bibb_Data_Mining_Final_Project.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Bs3xD21Fc5gn",
        "-hcnEOFffvxB"
      ],
      "toc_visible": true,
      "authorship_tag": "ABX9TyM2vlFN+9jY187D25rLbn0H",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MeghanBibb/PredictingAccidentSeverityDNN/blob/main/Meghan_Bibb_Data_Mining_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BD0ocFUDL1tO"
      },
      "source": [
        "The following code was used in the creation of [this paper](https://drive.google.com/file/d/1rOrnepeF9jt2GiKf3o6XsBayvTjz-POz/view?usp=sharing). "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrkQKBvB3l1d"
      },
      "source": [
        "## References\n",
        "\n",
        "How to import data to colab:\n",
        "https://towardsdatascience.com/importing-data-to-google-colab-the-clean-way-5ceef9e9e3c8\n",
        "\n",
        "How to plot a pie chart in matplotlib\n",
        "https://matplotlib.org/3.1.1/gallery/pie_and_polar_charts/pie_features.html\n",
        "\n",
        "One hot encoding https://machinelearningmastery.com/how-to-one-hot-encode-sequence-data-in-python/ \n",
        "\n",
        "Cramer's V\n",
        "https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
        "\n",
        "Data frame manipulation\n",
        "https://www.geeksforgeeks.org/python-creating-a-pandas-dataframe-column-based-on-a-given-condition/\n",
        "\n",
        "\n",
        "**Accademic Articles:**\n",
        "\n",
        "https://www.sciencedirect.com/science/article/pii/S2452247318302164 - Cramer's V Threshold explaination\n",
        "\n",
        "https://dlp-kdd.github.io/dlp-kdd2019/assets/pdf/a11-wu.pdf - Counting Features - an improvement on One-hot-encoding for large scale categorical data https://dl.acm.org/doi/10.1145/3326937.3341260\n",
        "\n",
        "Traffic Severity in South Africa:\n",
        "https://dl.acm.org/doi/pdf/10.1145/3325112.3325211?casa_token=yByFlPG5nJsAAAAA:Zt4du5F36pGjlG_R8RWcCp8ZaCJ3s7ep5j3rUShhGilOzQ4Sp8QqopcFkC8EdIV7iA10RuhvpwgwIg \n",
        "\n",
        "Accident predictions using big data https://dl.acm.org/doi/pdf/10.1145/3386723.3387886 \n",
        "\n",
        "The dataset itself: https://arxiv.org/abs/1906.05409\n",
        "\n",
        "Accident Risk Prediction based on Heterogeneous Sparse Data: https://arxiv.org/abs/1909.09638 \n",
        "\n",
        "https://arxiv.org/pdf/1909.10702.pdf - dimensionality reduction\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3leZTMOt3hvW"
      },
      "source": [
        "# Set Up"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XDg_5ax3N5-v"
      },
      "source": [
        "!pip install dython\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OVaYjzf-QwcP"
      },
      "source": [
        "**This colab mounts to your google drive to read files from. The files needed are located at: https://drive.google.com/drive/folders/1Yr04lpJ09MgbeqmHPq88aVBX7jOTAd__?usp=sharing  Please make a copy of this folder and place it in your drive.**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jUoeueow0pNF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b64ce855-7499-4185-9879-ea5ab00437a6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6zePyWzqSf3"
      },
      "source": [
        "#@title Please enter the path to the data\n",
        "pathToData = \"/content/drive/My Drive/Baylor/Data_Mining/Project/\" #@param {type:\"string\"}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xC9XoD997W3i"
      },
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pd0b_-KLubvb"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3W93IZ_8-h31"
      },
      "source": [
        "# Data Exploration & Visulization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rsD1g6F7aya"
      },
      "source": [
        "df = pd.read_csv(pathToData + 'TX_Accidents_June20.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bxHp2fFhWOi4"
      },
      "source": [
        "df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OaNE7nnxzVDi"
      },
      "source": [
        "plt.style.available"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jXgl2Q-U7yTL"
      },
      "source": [
        "severityCounts = df['Severity'].value_counts()\n",
        "\n",
        "plt.figure(figsize=(4.66,3))\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.bar(severityCounts.index, severityCounts.values, color='darkblue')\n",
        "#plt.title('Number of occurances for each severity classifcation', fontsize=16)\n",
        "plt.ylabel('Quantity of Accidents', fontsize=18)\n",
        "plt.xlabel('Severity Classifcation', fontsize=18)\n",
        "plt.xticks(np.arange(1, 5, step=1))\n",
        "plt.xticks(fontsize=18)\n",
        "plt.yticks(fontsize=18)\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d805qZh_mFM"
      },
      "source": [
        "## POI Visulizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iasUVFNwLd_q"
      },
      "source": [
        "arr = np.empty([0])\n",
        "\n",
        "poiCounts = df[df['Amenity'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Bump'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Crossing'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Give_Way'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Junction'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['No_Exit'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Railway'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Roundabout'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Station'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Stop'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Traffic_Calming'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Traffic_Signal'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "poiCounts = df[df['Turning_Loop'] == True]\n",
        "arr = np.append(arr, len(poiCounts))\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "\n",
        "POINames = ['Amenity', 'Bump', 'Crossing', 'Give_Way', 'Junction', 'No_Exit', \n",
        "          'Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal', 'Turning_Loop']\n",
        "\n",
        "POINamesLabels = np.array([0,1,2,3,4,5,6,7,8,9,10,11,12]);\n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "                tick.label.set_fontsize(18) \n",
        "plt.bar(POINamesLabels, arr, color='darkblue')\n",
        "fig.canvas.draw()\n",
        "\n",
        "plt.ylabel('Quantity of Accidents', fontsize=18)\n",
        "plt.xlabel('POI Locations', fontsize=18)\n",
        "ax.set_xticks(np.arange(0, len(POINamesLabels), step=1))\n",
        "\n",
        "ax.set_xticklabels(POINames, fontsize=18)\n",
        "plt.setp(ax.get_xticklabels(), rotation=90, ha=\"right\",\n",
        "         rotation_mode=\"anchor\")\n",
        "\n",
        "fig.set_size_inches(4.66,3)\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eu1nNHDaHnRX"
      },
      "source": [
        "\n",
        "## Weather Conditions\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "exfasCI2HmfB"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"Weather_Condition\", hue=\"Severity\", data=df, order=df.Weather_Condition.value_counts().iloc[:11].index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Accidents')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IMg1-V-EEmvc"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.countplot(x=\"Weather_Condition\", data=df, order=df.Weather_Condition.value_counts().iloc[:11].index)\n",
        "plt.xticks(rotation=90)\n",
        "plt.ylabel('Number of Accidents')\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g1uoOqSlnDe9"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(4.66,3))\n",
        "ax = sns.countplot(x=\"City\", data=df, order=df.City.value_counts().iloc[:6].index)\n",
        "plt.xticks(rotation=20, fontsize=15)\n",
        "plt.ylabel('Number of Accidents', fontsize=16)\n",
        "plt.yticks(fontsize=16)\n",
        "plt.xlabel('City', fontsize=16)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deMcM9kBIIWZ"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(4.66,3))\n",
        "ax = sns.countplot(x=\"Side\", hue=\"Severity\", data=df)\n",
        "ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, fontsize=18, title=\"Severity\")\n",
        "plt.ylabel('Number of Accidents', fontsize=18)\n",
        "plt.setp(ax.get_legend().get_texts(), fontsize='18') # for legend text\n",
        "plt.setp(ax.get_legend().get_title(), fontsize='18') # for legend title\n",
        "plt.yticks(fontsize=18)\n",
        "plt.xlabel('Side', fontsize=18)\n",
        "plt.xticks(fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXOep_bLuUQK"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(4.66,3))\n",
        "ax = sns.countplot(x=\"Sunrise_Sunset\", hue=\"Severity\", data=df)\n",
        "ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, fontsize=18, title=\"Severity\")\n",
        "plt.setp(ax.get_legend().get_texts(), fontsize='18') # for legend text\n",
        "plt.setp(ax.get_legend().get_title(), fontsize='18') # for legend title\n",
        "plt.yticks(fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.ylabel('Quantity of Accidents', fontsize=18)\n",
        "plt.xlabel('Sunrise_Sunset', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ikYtylDIX3r"
      },
      "source": [
        "import seaborn as sns\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(4.66,3))\n",
        "\n",
        "ax = sns.countplot(x=\"Source\", hue=\"Severity\", data=df,  order=df.Source.value_counts().iloc[:2].index)\n",
        "for tick in ax.yaxis.get_major_ticks():\n",
        "                tick.label.set_fontsize(18) \n",
        "for tick in ax.xaxis.get_major_ticks():\n",
        "                tick.label.set_fontsize(18)  \n",
        "\n",
        "ax.legend(loc='center right', bbox_to_anchor=(1.25, 0.5), ncol=1, fontsize=18, title=\"Severity\")\n",
        "\n",
        "plt.setp(ax.get_legend().get_texts(), fontsize='18') # for legend text\n",
        "plt.setp(ax.get_legend().get_title(), fontsize='18') # for legend title               \n",
        "\n",
        "plt.yticks(fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.ylabel('Number of Accidents', fontsize=18)\n",
        "plt.xlabel('Source', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3SW4-qJCI5vu"
      },
      "source": [
        "import seaborn as sns\n",
        "sns.set_theme(style=\"darkgrid\")\n",
        "ax = sns.scatterplot(x=\"Temperature(F)\", y=\"Severity\", data=df, hue=\"Severity\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1CxNJsSJ2qB"
      },
      "source": [
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(4.66,3))\n",
        "ax = sns.boxplot(x=\"Severity\", y=\"Temperature(F)\", hue=\"Severity\", data=df)\n",
        "ax.legend(loc='center right', bbox_to_anchor=(1.35, 0.5), ncol=1, fontsize=18, title=\"Severity\")\n",
        "plt.setp(ax.get_legend().get_texts(), fontsize='18') # for legend text\n",
        "plt.setp(ax.get_legend().get_title(), fontsize='18') # for legend title               \n",
        "\n",
        "plt.yticks(fontsize=18)\n",
        "plt.xticks(fontsize=18)\n",
        "plt.ylabel('Temerature(F)', fontsize=18)\n",
        "plt.xlabel('Severity', fontsize=18)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cbHan1uN_e0y"
      },
      "source": [
        "## Accidents per Hour Visulizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nb9joF_IcWVE"
      },
      "source": [
        "df4Severity = df[df['Severity'] == 4]\n",
        "df3Severity = df[df['Severity'] == 3]\n",
        "df2Severity = df[df['Severity'] == 2]\n",
        "df1Severity = df[df['Severity'] == 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4husu7_PmCsD"
      },
      "source": [
        "print(len(df1Severity))\n",
        "print(len(df2Severity))\n",
        "print(len(df3Severity))\n",
        "print(len(df4Severity))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rd7TWWzTpf3n"
      },
      "source": [
        "df[\"Start_Time_Hour\"] = df['Start_Time'].str.split(\" \",expand=True)[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pEip_wqTrEUJ"
      },
      "source": [
        "import time\n",
        "hours = pd.to_datetime(df[\"Start_Time_Hour\"]).dt.hour\n",
        "print(hours)\n",
        "print(np.unique(hours, return_counts=True))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MCu3__2apzJG"
      },
      "source": [
        "hourCounts = np.unique(hours, return_counts=True)\n",
        "\n",
        "plt.style.use('seaborn-whitegrid')\n",
        "plt.figure(figsize=(2.33,1.5))\n",
        "plt.bar(hourCounts[0], hourCounts[1], color='darkblue')\n",
        "\n",
        "#plt.title('Number of Accidents Per Hour', fontsize=16)\n",
        "plt.ylabel('Number of Accidents', fontsize=9)\n",
        "plt.xlabel('Time of Day (hours)', fontsize=9)\n",
        "plt.xticks(np.arange(0, 24, step=3), fontsize=9)\n",
        "plt.show()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "veRdVPznxvnm"
      },
      "source": [
        "s1hours = df1Severity['Start_Time'].str.split(\" \",expand=True)[1]\n",
        "s1CountsHours = np.unique(pd.to_datetime(s1hours).dt.hour, return_counts=True)\n",
        "s1label = np.linspace(1,1,len(s1CountsHours[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vPqZA8tkyegj"
      },
      "source": [
        "s2hours = df2Severity['Start_Time'].str.split(\" \",expand=True)[1]\n",
        "s2CountsHours = np.unique(pd.to_datetime(s2hours).dt.hour, return_counts=True)\n",
        "s2label = np.linspace(2,2,len(s2CountsHours[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uRm3yfCbylxw"
      },
      "source": [
        "s3hours = df3Severity['Start_Time'].str.split(\" \",expand=True)[1]\n",
        "s3CountsHours = np.unique(pd.to_datetime(s3hours).dt.hour, return_counts=True)\n",
        "s3label = np.linspace(3,3,len(s3CountsHours[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wj-0evVByptp"
      },
      "source": [
        "s4hours = df4Severity['Start_Time'].str.split(\" \",expand=True)[1]\n",
        "s4CountsHours = np.unique(pd.to_datetime(s4hours).dt.hour, return_counts=True)\n",
        "s4label = np.linspace(4,4,len(s4CountsHours[0]))\n",
        "s4label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6k5ndZ-yytws"
      },
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "\n",
        "scalar = 1/10\n",
        "plt.scatter(s1CountsHours[0],s1label,s=s1CountsHours[1]*scalar, color='black')\n",
        "plt.scatter(s2CountsHours[0],s2label,s=s2CountsHours[1]*scalar, color='orange')\n",
        "plt.scatter(s3CountsHours[0],s3label,s=s3CountsHours[1]*scalar, color='red')\n",
        "plt.scatter(s4CountsHours[0],s4label,s=s4CountsHours[1]*scalar, color='darkred')\n",
        "\n",
        "plt.xticks(np.arange(0, 24, step=1))\n",
        "\n",
        "plt.title('Severe Accidents per Hour', fontsize=16)\n",
        "plt.ylabel('The severity of the accident', fontsize=12)\n",
        "plt.xlabel('Hours in a day (military time)', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "68nNWPhM49LL"
      },
      "source": [
        "See the distrobution of accidents over each month"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g7HgPicX_Oja"
      },
      "source": [
        "## Accidents per Month per Severity Visulization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CTkJ79RJ4O6e"
      },
      "source": [
        "s1date = df1Severity['Start_Time'].str.split(\" \",expand=True)[0]\n",
        "s1CountsMonths = np.unique(pd.to_datetime(s1date).dt.month, return_counts=True)\n",
        "s1monthlabel = np.linspace(1,1,len(s1CountsMonths[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoADMPou4b9B"
      },
      "source": [
        "s2date = df2Severity['Start_Time'].str.split(\" \",expand=True)[0]\n",
        "s2CountsMonths = np.unique(pd.to_datetime(s2date).dt.month, return_counts=True)\n",
        "s2monthlabel = np.linspace(2,2,len(s2CountsMonths[0]))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qUJPAtHM8x9Z"
      },
      "source": [
        "s3date = df3Severity['Start_Time'].str.split(\" \",expand=True)[0]\n",
        "s3CountsMonths = np.unique(pd.to_datetime(s3date).dt.month, return_counts=True)\n",
        "s3monthlabel = np.linspace(3,3,len(s3CountsMonths[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GH9WQ1DC9Amr"
      },
      "source": [
        "s4date = df4Severity['Start_Time'].str.split(\" \",expand=True)[0]\n",
        "s4CountsMonths = np.unique(pd.to_datetime(s3date).dt.month, return_counts=True)\n",
        "s4monthlabel = np.linspace(4,4,len(s3CountsMonths[0]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DIJx-Jz49Izg"
      },
      "source": [
        "plt.figure(figsize=(10,6))\n",
        "\n",
        "scalar = 1/10\n",
        "plt.scatter(s1CountsMonths[0],s1monthlabel,s=s1CountsMonths[1]*scalar, color='black')\n",
        "plt.scatter(s2CountsMonths[0],s2monthlabel,s=s2CountsMonths[1]*scalar, color='orange')\n",
        "plt.scatter(s3CountsMonths[0],s3monthlabel,s=s3CountsMonths[1]*scalar, color='red')\n",
        "plt.scatter(s4CountsMonths[0],s4monthlabel,s=s4CountsMonths[1]*scalar, color='darkred')\n",
        "\n",
        "plt.xticks(np.arange(1, 12, step=1))\n",
        "plt.yticks(np.arange(1, 5, step=1))\n",
        "\n",
        "plt.title('Severe Accidents per Month', fontsize=16)\n",
        "plt.ylabel('The severity of the accident', fontsize=12)\n",
        "plt.xlabel('Month', fontsize=12)\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vLzaUQnFrNyR"
      },
      "source": [
        "## Geopandas Spacial Visulizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nPvpgqQeZg8G"
      },
      "source": [
        "!pip install geopandas"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IEiJYYzSZbdZ"
      },
      "source": [
        "from shapely.geometry import Point\n",
        "import geopandas as gpd\n",
        "from geopandas import GeoDataFrame"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ot-b4Y9wcRya"
      },
      "source": [
        "states = gpd.read_file(pathToData + 'usa-states-census-2014.shp')\n",
        "type(states)\n",
        "\n",
        "# Fix found on : https://stackoverflow.com/questions/38961816/geopandas-set-crs-on-points\n",
        "states.set_crs(epsg=4326, inplace=True)\n",
        "\n",
        "# project to merkator\n",
        "states.to_crs(epsg=3395)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QR4d3JwsfPB_"
      },
      "source": [
        "# Based on the tutorial: https://jcutrer.com/python/learn-geopandas-plotting-usmaps and http://jonathansoma.com/lede/foundations-2017/classes/geopandas/mapping-with-geopandas/\n",
        "geometry = [Point(xy) for xy in zip(df['Start_Lng'], df['Start_Lat'])]\n",
        "gdf = GeoDataFrame(df, geometry=geometry)   \n",
        "\n",
        "#this is a simple map that goes with geopandas\n",
        "\n",
        "gdf.plot(ax=states[states['NAME'] == 'Texas'].plot(color='lightgrey', linewidth=0.5, edgecolor='white', figsize=(2.3, 1.5)), markersize=1, column='Severity', alpha=0.5, cmap='plasma', legend=True)\n",
        "plt.ylabel('Latitude', fontsize=9)\n",
        "plt.xlabel('Longitude', fontsize=9)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu1jLNCT_Fho"
      },
      "source": [
        "## Correlation Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SBq533qeAG80"
      },
      "source": [
        "I want to measure the correlation between severity and all numeric features. I want to measure the association between categorical features and severity.\n",
        "\n",
        "\n",
        "References: https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/23849 "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MzKi1JbjQc_T"
      },
      "source": [
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oMt-5Sjcqupw"
      },
      "source": [
        "np.corrcoef(df['Severity'], df['Distance(mi)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z30kQa7wOrBL"
      },
      "source": [
        "tempDF = df.dropna(subset=['Temperature(F)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Temperature(F)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WaNXl31gPDZ9"
      },
      "source": [
        "tempDF = df.dropna(subset=['Wind_Chill(F)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Wind_Chill(F)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zxRetnQlP5pL"
      },
      "source": [
        "tempDF = df.dropna(subset=['Humidity(%)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Humidity(%)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5S5K4txiQCTR"
      },
      "source": [
        "tempDF = df.dropna(subset=['Pressure(in)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Pressure(in)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i6xoNSounKNn"
      },
      "source": [
        "tempDF = df.dropna(subset=['Wind_Speed(mph)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Wind_Speed(mph)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bviMGRvfnhS3"
      },
      "source": [
        "tempDF = df.dropna(subset=['Precipitation(in)'])\n",
        "np.corrcoef(tempDF['Severity'], tempDF['Precipitation(in)'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lznnHw4smTgL"
      },
      "source": [
        "### Experiments with Cramer's V and Categorical Associations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWvEgaRKcqKd"
      },
      "source": [
        "# I learned about Cramer's V at https://towardsdatascience.com/the-search-for-categorical-correlation-a1cf7f1888c9\n",
        "# I also borrowed this function from that article\n",
        "# Measure of association between categorical values\n",
        "# Similarly to correlation, the output is in the range of [0,1], where 0 means no association and 1 is full association. \n",
        "# (Unlike correlation, there are no negative values, as there’s no such thing as a negative association. Either there is, or there isn’t)\n",
        "def cramers_v(x, y):\n",
        "    confusion_matrix = pd.crosstab(x,y)\n",
        "    chi2 = ss.chi2_contingency(confusion_matrix)[0]\n",
        "    n = confusion_matrix.sum().sum()\n",
        "    phi2 = chi2/n\n",
        "    r,k = confusion_matrix.shape\n",
        "    phi2corr = max(0, phi2-((k-1)*(r-1))/(n-1))\n",
        "    rcorr = r-((r-1)**2)/(n-1)\n",
        "    kcorr = k-((k-1)**2)/(n-1)\n",
        "    return np.sqrt(phi2corr/min((kcorr-1),(rcorr-1)))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92fVAPXkcrq_"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as ss\n",
        "import seaborn as sns\n",
        "windDirDF = df.dropna(subset=['Wind_Direction'])\n",
        "cramers_v(windDirDF['Severity'], windDirDF['Wind_Direction'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I0ERMMkyerqH"
      },
      "source": [
        "sns.heatmap(pd.crosstab(windDirDF.loc[df['Severity'] == 1]['Severity'], windDirDF.loc[df['Severity'] == 1]['Wind_Direction']))\n",
        "plt.figure()\n",
        "sns.heatmap(pd.crosstab(windDirDF.loc[df['Severity'] == 2]['Severity'], windDirDF.loc[df['Severity'] == 2]['Wind_Direction']))\n",
        "plt.figure()\n",
        "sns.heatmap(pd.crosstab(windDirDF.loc[df['Severity'] == 3]['Severity'], windDirDF.loc[df['Severity'] == 3]['Wind_Direction']))\n",
        "plt.figure()\n",
        "sns.heatmap(pd.crosstab(windDirDF.loc[df['Severity'] == 4]['Severity'], windDirDF.loc[df['Severity'] == 4]['Wind_Direction']))\n",
        "plt.figure()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bJGtU474fkFv"
      },
      "source": [
        "tempDF = df.dropna(subset=['Sunrise_Sunset'])\n",
        "cramers_v(tempDF['Severity'], tempDF['Sunrise_Sunset'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cgolxti8x2CT"
      },
      "source": [
        "tempDF = df.dropna(subset=['Civil_Twilight'])\n",
        "cramers_v(tempDF['Severity'], tempDF['Civil_Twilight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jfmeHmVSx2mi"
      },
      "source": [
        "tempDF = df.dropna(subset=['Nautical_Twilight'])\n",
        "cramers_v(tempDF['Severity'], tempDF['Nautical_Twilight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_w0j2o82yDRc"
      },
      "source": [
        "tempDF = df.dropna(subset=['Astronomical_Twilight'])\n",
        "cramers_v(tempDF['Severity'], tempDF['Astronomical_Twilight'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "96IDcobVW6kh"
      },
      "source": [
        "### Dython Association comparisons"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9d8NcnXTYT4Y"
      },
      "source": [
        "df['Severity'] = df['Severity'].astype(str)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tIjto5dvHxyS"
      },
      "source": [
        "from dython.nominal import associations\n",
        "associations(df.loc[:,('Severity', 'Civil_Twilight', 'Sunrise_Sunset', 'Nautical_Twilight', 'Astronomical_Twilight')])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RfRW53kNIirB"
      },
      "source": [
        "associations(df.loc[:,('Severity', 'Amenity', 'Bump', 'Give_Way', 'Junction', 'No_Exit','Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal')], figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlRgG8qIKyFi"
      },
      "source": [
        "associations(df.loc[:,('Severity','Temperature(F)', 'Wind_Chill(F)', 'Humidity(%)', 'Pressure(in)', 'Visibility(mi)', 'Wind_Direction','Wind_Speed(mph)', 'Precipitation(in)', 'Weather_Condition')], figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MIJ6M--Mj26"
      },
      "source": [
        "associations(df.loc[:,('Severity', 'Airport_Code', 'Timezone', 'County', 'City', 'Side')], figsize=(10,10))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8qPpqs7M-UR"
      },
      "source": [
        "df['Duration_Length'] = (pd.to_datetime(df['End_Time'])-pd.to_datetime(df['Start_Time'])).astype('timedelta64[m]')\n",
        "associations(df.loc[:,('Severity', 'Distance(mi)', 'Start_Lat', 'Start_Lng', 'Duration_Length','TMC', 'Source')], figsize=(10,10))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HIsnsIYwZHzG"
      },
      "source": [
        "associations(df.loc[:,('Severity', 'Distance(mi)', 'Start_Lat', 'Start_Lng', 'Duration_Length','TMC', 'Source')], figsize=(10,10))  "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJpjLlGId6pr"
      },
      "source": [
        "It seems the geographical location features were most correlated/associated with severity. Factors such as distance, side, weather condition, and the location of the accident being a traffic stop also had a correlation/association value above |.1|. Correlation values range from (-1,1) whereas association values range from (0,1). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Bs3xD21Fc5gn"
      },
      "source": [
        "# Prepare Data for Models"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syjTrjYLdTW5"
      },
      "source": [
        "After looking at the above dython association comparisons it seems like many of these features are not helpful in predicting severity. I want to plan on using a threshold of .1, .15, and .2 to use only values that have at least an |n| association or correlation with Severity. \n",
        "\n",
        "In order to use these values in a model or autoencoder, they must be scaled and prepared to work best with TensorFlow. \n",
        "\n",
        "I also want to look at the natural language data features which I was not able to correlate or find association metrics for. I plan to use TFX to gather meaning behind the words as well as a TF-IDF to find potentially important words relating to the accidents and their severity. Applying NLP to the weather condition categories might be interesting as well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yWcCY7mHj0W0"
      },
      "source": [
        "Data that exists at a |.1| threshold or higher includes distance, side, city, state, county, airport code, and weather condition. I expect that city, county, and airport code may provide similar information and may not all be needed. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ttUUpDmgUwO"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib as plt\n",
        "from sklearn.utils import shuffle\n",
        "\n",
        "df = pd.read_csv(pathToData + 'TX_Accidents_June20.csv')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZMq-ljuv9FA8"
      },
      "source": [
        "X = df "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f-T3HER1R1c7"
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "colsWeCareAbout = [ 'Distance(mi)', 'Weather_Condition', 'Airport_Code', 'County', 'City', 'Side', 'Amenity', 'Bump', 'Give_Way', 'Junction', 'No_Exit','Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
        "categoricalCols =  ['Weather_Condition', 'County', 'City', 'Side', 'Airport_Code']\n",
        "booleanCols = ['Amenity', 'Bump', 'Give_Way', 'Junction', 'No_Exit','Railway', 'Roundabout', 'Station', 'Stop', 'Traffic_Calming', 'Traffic_Signal']\n",
        "X_ = X.loc[:,( colsWeCareAbout )]\n",
        "\n",
        "y = X.loc[:,('Severity')]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CydIjG6BRpGv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "922cf373-f103-44a3-c485-cef1471941fb"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "inpt_dim = X_.shape[1]\n",
        "x_train, x_test, y_train, y_test = train_test_split(X_, y, test_size=0.4, stratify=y, random_state=42)\n",
        "x_test, x_validate, y_test, y_validate = train_test_split(x_test, y_test, test_size=0.5, random_state=42)\n",
        "\n",
        "y_train_values = y_train\n",
        "y_test_values = y_test\n",
        "y_test = pd.get_dummies(y_test)\n",
        "y_train = pd.get_dummies(y_train)\n",
        "\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "print(x_validate.shape, y_validate.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(197570, 17) (197570, 4)\n",
            "(65857, 17) (65857, 4)\n",
            "(65857, 17) (65857,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hWTNbeQIwuzw",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "b90421ec-3dc5-43a6-e630-10ea27482d97"
      },
      "source": [
        "x_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Distance(mi)</th>\n",
              "      <th>Weather_Condition</th>\n",
              "      <th>Airport_Code</th>\n",
              "      <th>County</th>\n",
              "      <th>City</th>\n",
              "      <th>Side</th>\n",
              "      <th>Amenity</th>\n",
              "      <th>Bump</th>\n",
              "      <th>Give_Way</th>\n",
              "      <th>Junction</th>\n",
              "      <th>No_Exit</th>\n",
              "      <th>Railway</th>\n",
              "      <th>Roundabout</th>\n",
              "      <th>Station</th>\n",
              "      <th>Stop</th>\n",
              "      <th>Traffic_Calming</th>\n",
              "      <th>Traffic_Signal</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>259959</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Clear</td>\n",
              "      <td>KRYW</td>\n",
              "      <td>Travis</td>\n",
              "      <td>Lago Vista</td>\n",
              "      <td>L</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>60126</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Clear</td>\n",
              "      <td>KAUS</td>\n",
              "      <td>Travis</td>\n",
              "      <td>Austin</td>\n",
              "      <td>L</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23248</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Scattered Clouds</td>\n",
              "      <td>KRYW</td>\n",
              "      <td>Travis</td>\n",
              "      <td>Cedar Park</td>\n",
              "      <td>L</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>319455</th>\n",
              "      <td>1.436</td>\n",
              "      <td>Partly Cloudy</td>\n",
              "      <td>KSGR</td>\n",
              "      <td>Harris</td>\n",
              "      <td>Houston</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>325760</th>\n",
              "      <td>0.477</td>\n",
              "      <td>Partly Cloudy</td>\n",
              "      <td>KRBD</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>Dallas</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>104360</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Mostly Cloudy</td>\n",
              "      <td>KMCJ</td>\n",
              "      <td>Harris</td>\n",
              "      <td>Houston</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>185881</th>\n",
              "      <td>0.110</td>\n",
              "      <td>Clear</td>\n",
              "      <td>KATT</td>\n",
              "      <td>Travis</td>\n",
              "      <td>Austin</td>\n",
              "      <td>R</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>172898</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Scattered Clouds</td>\n",
              "      <td>KSAT</td>\n",
              "      <td>Bexar</td>\n",
              "      <td>San Antonio</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>202712</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Partly Cloudy</td>\n",
              "      <td>KAUS</td>\n",
              "      <td>Travis</td>\n",
              "      <td>Austin</td>\n",
              "      <td>L</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>303289</th>\n",
              "      <td>0.000</td>\n",
              "      <td>Partly Cloudy</td>\n",
              "      <td>KDFW</td>\n",
              "      <td>Tarrant</td>\n",
              "      <td>Euless</td>\n",
              "      <td>R</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>True</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "      <td>False</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>197570 rows × 17 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        Distance(mi) Weather_Condition  ... Traffic_Calming Traffic_Signal\n",
              "259959         0.000             Clear  ...           False          False\n",
              "60126          0.000             Clear  ...           False          False\n",
              "23248          0.000  Scattered Clouds  ...           False          False\n",
              "319455         1.436     Partly Cloudy  ...           False          False\n",
              "325760         0.477     Partly Cloudy  ...           False          False\n",
              "...              ...               ...  ...             ...            ...\n",
              "104360         0.000     Mostly Cloudy  ...           False          False\n",
              "185881         0.110             Clear  ...           False           True\n",
              "172898         0.000  Scattered Clouds  ...           False           True\n",
              "202712         0.000     Partly Cloudy  ...           False          False\n",
              "303289         0.000     Partly Cloudy  ...           False          False\n",
              "\n",
              "[197570 rows x 17 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "22SFSx-az3-9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "890ee9b5-e619-4dac-9ec9-bec2b52b94a4"
      },
      "source": [
        "y_train_values"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "259959    2\n",
              "60126     2\n",
              "23248     2\n",
              "319455    4\n",
              "325760    2\n",
              "         ..\n",
              "104360    2\n",
              "185881    2\n",
              "172898    2\n",
              "202712    2\n",
              "303289    3\n",
              "Name: Severity, Length: 197570, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-hcnEOFffvxB"
      },
      "source": [
        "## Scaling of Numeric Data\n",
        "\n",
        "Numeric data passing the |.1| correlation/association threshold:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DAE_ZpTGhw-b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4c6ecfe-5e18-4f69-abea-56929ea6fa5c"
      },
      "source": [
        "x_train['Distance(mi)']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "259959    0.000\n",
              "60126     0.000\n",
              "23248     0.000\n",
              "319455    1.436\n",
              "325760    0.477\n",
              "          ...  \n",
              "104360    0.000\n",
              "185881    0.110\n",
              "172898    0.000\n",
              "202712    0.000\n",
              "303289    0.000\n",
              "Name: Distance(mi), Length: 197570, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PnNSsqZgiMTB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7900b34-3ab5-438e-8b17-8287e3738c63"
      },
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "scaler = StandardScaler()\n",
        "x_train[['Distance(mi)']] = scaler.fit_transform(x_train[['Distance(mi)']])\n",
        "x_test[['Distance(mi)']] = scaler.transform(x_test[['Distance(mi)']])\n",
        "x_validate[['Distance(mi)']] = scaler.transform(x_validate[['Distance(mi)']])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:3: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  This is separate from the ipykernel package so we can avoid doing imports until\n",
            "/usr/local/lib/python3.6/dist-packages/pandas/core/indexing.py:1734: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame.\n",
            "Try using .loc[row_indexer,col_indexer] = value instead\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  isetter(loc, value[:, i].tolist())\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W9BCqQ9oi7hG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aef65d7c-fbe3-4427-c249-41793e1a7310"
      },
      "source": [
        "x_train['Distance(mi)']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "259959   -0.173329\n",
              "60126    -0.173329\n",
              "23248    -0.173329\n",
              "319455    2.024026\n",
              "325760    0.556572\n",
              "            ...   \n",
              "104360   -0.173329\n",
              "185881   -0.005008\n",
              "172898   -0.173329\n",
              "202712   -0.173329\n",
              "303289   -0.173329\n",
              "Name: Distance(mi), Length: 197570, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ4IooxbkKxR"
      },
      "source": [
        "## Categorical Data Preparation\n",
        "\n",
        "One-hot-encoding does not work effectively (spatially) with data such as cities, because there are just so many it breaks the barriers of the ammount of space each one hot encoded vector should be. Instead, the Counting Features approach will be used representing each categorical variable as teo values, its frequency and its average target value.\n",
        "\n",
        "This will include the preparation of side, city, county, airport code, and weather condition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dxVRsxT8KiCW"
      },
      "source": [
        "f_freq = how many occurances of category x / # of rows\n",
        "\n",
        "f_avg = # of x's in row with target i / # of rows with target i"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZjJak04znQrd"
      },
      "source": [
        "x_train = pd.get_dummies(x_train, columns = categoricalCols).astype(float)\n",
        "x_test = pd.get_dummies(x_test, columns = categoricalCols).astype(float)\n",
        "x_validate = pd.get_dummies(x_validate, columns = categoricalCols).astype(float)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O16KQXHLsCjv"
      },
      "source": [
        "# Ensures one hot encoded columns are the same for the test and train dataset.\n",
        "# https://stackoverflow.com/questions/41335718/keep-same-dummy-variable-in-training-and-testing-data \n",
        "\n",
        "missing_cols = set( x_train.columns ) - set( x_test.columns )\n",
        "# Add a missing column in test set with default value equal to 0\n",
        "for c in missing_cols:\n",
        "    x_test[c] = 0\n",
        "# Ensure the order of column in the test set is in the same order than in train set\n",
        "x_test = x_test[x_train.columns]\n",
        "\n",
        "\n",
        "missing_cols = set( x_train.columns ) - set( x_validate.columns )\n",
        "# Add a missing column in validation set with default value equal to 0\n",
        "for c in missing_cols:\n",
        "    x_validate[c] = 0\n",
        "# Ensure the order of column in the validation set is in the same order than in train set\n",
        "x_validate = x_validate[x_train.columns]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJNImuQawGh1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e2d73e9-43ae-4da5-e118-eaff58ce18f7"
      },
      "source": [
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)\n",
        "print(x_validate.shape, y_validate.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(197570, 1108) (197570, 4)\n",
            "(65857, 1108) (65857, 4)\n",
            "(65857, 1108) (65857,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UydruWYBU5RG"
      },
      "source": [
        "https://www.kite.com/python/answers/how-to-convert-a-pandas-dataframe-into-a-list-of-tuples-in-python use this\n",
        "\n",
        "Why I don't translate my binary features to one hot encoded ones https://stackoverflow.com/questions/43515877/should-binary-features-be-one-hot-encoded "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgSikJ3gKlmi"
      },
      "source": [
        "This VAE was meant to generate more data for accidents with severity 1-4. It doesn't work"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dsBTYGL7U22r",
        "cellView": "code"
      },
      "source": [
        "from tensorflow.keras import backend as K\n",
        "\n",
        "def sampling(z_params):\n",
        "  z_mean, z_log_var = z_params\n",
        "  batch = K.shape(z_mean)[0]\n",
        "  dims = K.int_shape(z_mean)[1]\n",
        "  epsilon = K.random_normal(shape=(batch, dims)) \n",
        "  return z_mean + K.exp(0.5 * z_log_var) * epsilon"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QFYc2MI3VykP"
      },
      "source": [
        "y_train_tmp = y_train\n",
        "y_train_tmp = y_train_tmp.rename(columns={1: \"Severity_1\", 2: \"Severity_2\", 3: \"Severity_3\", 4: \"Severity_4\"}, errors=\"raise\")\n",
        "\n",
        "y_test_tmp = y_test\n",
        "y_test_tmp = y_test_tmp.rename(columns={1: \"Severity_1\", 2: \"Severity_2\", 3: \"Severity_3\", 4: \"Severity_4\"}, errors=\"raise\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0nA6rL5qaDiH"
      },
      "source": [
        "train_14_x = pd.concat([x_train, y_train_tmp], axis=1, sort=False)\n",
        "train1s = train_14_x[ train_14_x[\"Severity_1\"] == 1]\n",
        "train4s = train_14_x[ train_14_x[\"Severity_4\"] == 1]\n",
        "\n",
        "train_14_x = pd.concat([train1s, train4s], axis=0, sort=False)\n",
        "train_14_x = train_14_x.drop(columns=[\"Severity_1\",\"Severity_2\",\"Severity_3\",\"Severity_4\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQOFohtQvmwE"
      },
      "source": [
        "test_14_x = pd.concat([x_test, y_test_tmp], axis=1, sort=False)\n",
        "\n",
        "test1s = test_14_x[test_14_x[\"Severity_1\"] == 1]\n",
        "test4s = test_14_x[test_14_x[\"Severity_4\"] == 1]\n",
        "\n",
        "test_14_x = pd.concat([test1s, test4s], axis=0, sort=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "99vm0UrRYZOA"
      },
      "source": [
        "test_14_y = test_14_x[[\"Severity_1\", \"Severity_4\"]]\n",
        "test_14_x =test_14_x.drop(columns=[\"Severity_1\",\"Severity_2\",\"Severity_3\",\"Severity_4\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t6wizKQ3kJmv",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "30e3fe8b-c197-4511-e816-b5df3d7f2b40"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Input\n",
        "from tensorflow.keras.layers import BatchNormalization, Lambda\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "inpt_dim = train_14_x.shape[1]\n",
        "original_dim = inpt_dim\n",
        "ltnt_dim = 2\n",
        "\n",
        "input_shape = (inpt_dim, )\n",
        "intermediate_dim = 512\n",
        "batch_size = 1024\n",
        "latent_dim = 128\n",
        "epochs = 50\n",
        "\n",
        "inputs = Input(shape=input_shape, name='encoder_input')\n",
        "l1 = Dense(intermediate_dim, activation='relu')(inputs)\n",
        "z_mean = Dense(latent_dim, name='z_mean')(l1)\n",
        "z_log_var = Dense(latent_dim, name='z_log_var')(l1)\n",
        "\n",
        "z = Lambda(sampling, output_shape=(latent_dim,), name='z')([z_mean, z_log_var])\n",
        "\n",
        "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
        "encoder.summary()\n",
        "\n",
        "latent_inputs = Input(shape=(latent_dim,), name='z_sampling')\n",
        "l2 = Dense(intermediate_dim, activation='relu')(latent_inputs)\n",
        "outputs = Dense(original_dim, activation='sigmoid')(l2)\n",
        "\n",
        "decoder = Model(latent_inputs, outputs, name='decoder')\n",
        "decoder.summary()\n",
        "\n",
        "outputs = decoder(encoder(inputs)[2])\n",
        "vae = Model(inputs, outputs, name='vae_mlp')\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"encoder\"\n",
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "encoder_input (InputLayer)      [(None, 1108)]       0                                            \n",
            "__________________________________________________________________________________________________\n",
            "dense (Dense)                   (None, 512)          567808      encoder_input[0][0]              \n",
            "__________________________________________________________________________________________________\n",
            "z_mean (Dense)                  (None, 128)          65664       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z_log_var (Dense)               (None, 128)          65664       dense[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "z (Lambda)                      (None, 128)          0           z_mean[0][0]                     \n",
            "                                                                 z_log_var[0][0]                  \n",
            "==================================================================================================\n",
            "Total params: 699,136\n",
            "Trainable params: 699,136\n",
            "Non-trainable params: 0\n",
            "__________________________________________________________________________________________________\n",
            "Model: \"decoder\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "z_sampling (InputLayer)      [(None, 128)]             0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 512)               66048     \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              (None, 1108)              568404    \n",
            "=================================================================\n",
            "Total params: 634,452\n",
            "Trainable params: 634,452\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKG4Tc2DJ5DX"
      },
      "source": [
        "# AE for Dimensionality Reduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkaDuSq4LQVX"
      },
      "source": [
        "This Autoencoder is meant to reduce the dimensionality of the dataset from 1108 dimensions to 256 by using the trained encoder."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0-itHCWK4k7"
      },
      "source": [
        "from tensorflow import keras\n",
        "from tensorflow.keras.datasets import cifar10\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Dropout, Activation, Input\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.python.keras import regularizers\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "import numpy as np\n",
        "\n",
        "inpt_dim =  1108\n",
        "ltnt_dim = 256\n",
        "\n",
        "# The data, split between train and test sets:\n",
        "print('x_train shape:', x_train.shape)\n",
        "print('x_test shape:', x_test.shape)\n",
        "\n",
        "inpt_vec = Input(shape=(inpt_dim,))\n",
        "el1 = Dense(2048, kernel_regularizer=regularizers.l1(0.0001))(inpt_vec)\n",
        "el2 = Activation('relu')(el1)\n",
        "el3 = Dense(1024)(el2)\n",
        "el4 = BatchNormalization()(el3)\n",
        "el5 = Activation('relu')(el4)\n",
        "el6 = Dropout(0.2)(el5)\n",
        "\n",
        "el7 = Dense(512, kernel_regularizer=regularizers.l1(0.0001))(el6)\n",
        "el8 = Activation('relu')(el7)\n",
        "el9 = Dense(512)(el8)\n",
        "el10 = BatchNormalization()(el9)\n",
        "el11 = Activation('relu')(el10)\n",
        "el12 = Dropout(0.2)(el11)\n",
        "\n",
        "el13 = Dense(256, kernel_regularizer=regularizers.l1(0.0001))(el12)\n",
        "el14 = Activation('relu')(el13)\n",
        "el15 = Dropout(0.2)(el14)\n",
        "el16 = Dense(ltnt_dim)(el15)\n",
        "el17 = BatchNormalization()(el16)\n",
        "encoder = Activation('tanh')(el17)\n",
        "\n",
        "# model that takes input and encodes it into the latent space\n",
        "latent_ncdr = Model(inpt_vec, encoder)\n",
        "\n",
        "dl1 = Dense(256, kernel_regularizer=regularizers.l1(0.0001))(encoder)\n",
        "dl2 = BatchNormalization()(dl1)\n",
        "dl3 = Activation('relu')(dl2)\n",
        "\n",
        "dl4 = Dropout(0.2)(dl3)\n",
        "dl5 = Dense(512)(dl4)\n",
        "dl6 = Activation('relu')(dl5)\n",
        "dl7 = Dense(512, kernel_regularizer=regularizers.l1(0.0001))(dl6)\n",
        "dl8 = BatchNormalization()(dl7)\n",
        "dl9 = Activation('relu')(dl8)\n",
        "\n",
        "dl10 = Dropout(0.2)(dl9)\n",
        "dl11 = Dense(1024)(dl10)\n",
        "dl12 = Activation('relu')(dl11)\n",
        "dl13 = Dense(2048, kernel_regularizer=regularizers.l1(0.0001))(dl12)\n",
        "dl14 = BatchNormalization()(dl13)\n",
        "dl15 = Activation('relu')(dl14)\n",
        "decoder = Dense(inpt_dim, activation='sigmoid') (dl15)\n",
        "\n",
        "# model that takes input, encodes it, and decodes it\n",
        "autoencoder = Model(inpt_vec, decoder)\n",
        "\n",
        "# setup RMSprop optimizer\n",
        "opt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6, )\n",
        "\n",
        "# autoencoder.compile(loss='binary_crossentropy', optimizer='adam')\n",
        "autoencoder.compile(loss='binary_crossentropy', optimizer=opt)\n",
        "\n",
        "hist = autoencoder.fit(x_train, x_train, epochs=200, batch_size=1000, \n",
        "                       shuffle=True, validation_data=(x_test, x_test))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jfz2F1JAOJ8-"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "fig = plt.figure(figsize=(10,6))\n",
        "plt.plot(hist.history['loss'], color='#785ef0')\n",
        "plt.plot(hist.history['val_loss'], color='#dc267f')\n",
        "plt.title('Model reconstruction loss')\n",
        "plt.ylabel('Brinary Cross-Entropy Loss')\n",
        "plt.xlabel('Epoch')\n",
        "plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
        "plt.savefig('AEloss.png', dpi=350, bbox_inches='tight')\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "we_CzidRS9US"
      },
      "source": [
        "latent_ncdr.save(\"latent_ncdr.hdf5\")\n",
        "autoencoder.save(\"autoencoder.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRdEGW9e-BhG"
      },
      "source": [
        "##UMAP Visualizations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7vr3U1XRS8DQ",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 354
        },
        "outputId": "ae90ffc1-b429-4549-adf2-cab7bc425db1"
      },
      "source": [
        "from tensorflow import keras\n",
        "latent_ncdr = keras.models.load_model('latent_ncdr.hdf5')\n",
        "autoencoder = keras.models.load_model('autoencoder.hdf5')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-a02533c6f4bb>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mlatent_ncdr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'latent_ncdr.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mautoencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'autoencoder.hdf5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    209\u001b[0m       \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m         \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    112\u001b[0m                   (export_dir,\n\u001b[1;32m    113\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: autoencoder.hdf5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RqgxF1DCOT-j",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 178
        },
        "outputId": "26299fc6-15f1-436b-c509-e731620d250b"
      },
      "source": [
        "encdd = latent_ncdr.predict(x_validate)\n",
        "x_hat = autoencoder.predict(x_validate)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-3ce3865f74e3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mencdd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlatent_ncdr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mx_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautoencoder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_validate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'autoencoder' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-42l1GDrCva"
      },
      "source": [
        "UMAP Visualization of the encoded validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jgcr5X_mOGDe"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "y_umap = list(map(int, y_validate))\n",
        "\n",
        "print(encdd.shape)\n",
        "\n",
        "encdd = umap.UMAP().fit_transform(encdd)\n",
        "print(encdd.shape)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Zu-DhvulOZi-"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "plt.title('UMAP of a 256-Layer Encoder')\n",
        "plt.scatter(encdd[:,0], encdd[:,1], s=10.0, c=y_umap, alpha=0.75, cmap='inferno')\n",
        "plt.xlabel('First UMAP dimension')\n",
        "plt.ylabel('Second UMAP dimension')\n",
        "plt.colorbar()\n",
        "plt.savefig('AEumap.png', bbox_inches='tight', dpi=350)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iG8e_6YhrLIl"
      },
      "source": [
        "UMAP Visualization of the Encoded Test Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FqVTE8HgrIQw"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "y_umap = list(map(int, y_test_values))\n",
        "encdd = latent_ncdr.predict(x_test)\n",
        "\n",
        "print(encdd.shape)\n",
        "\n",
        "encdd = umap.UMAP().fit_transform(encdd)\n",
        "print(encdd.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v5O-HRCrKSO"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "plt.title('UMAP of a 256-Layer Encoder')\n",
        "plt.scatter(encdd[:,0], encdd[:,1], s=10.0, c=y_umap, alpha=0.75, cmap='inferno')\n",
        "plt.xlabel('First UMAP dimension')\n",
        "plt.ylabel('Second UMAP dimension')\n",
        "plt.colorbar()\n",
        "plt.savefig('AEumap.png', bbox_inches='tight', dpi=350)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iF7i4vwqq7R_"
      },
      "source": [
        "Umap visualization of the original validation data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DRl1-IXmVFtU"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import umap\n",
        "\n",
        "y_umap = list(map(int, y_validate))\n",
        "\n",
        "print(x_validate.shape)\n",
        "\n",
        "original = umap.UMAP().fit_transform(x_validate)\n",
        "print(original.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X5elKrhPVLdz"
      },
      "source": [
        "import seaborn as sns\n",
        "\n",
        "plt.figure(figsize=(10,8))\n",
        "sns.set_style(\"white\")\n",
        "\n",
        "plt.title('UMAP of Orignal Data')\n",
        "plt.scatter(original[:,0], original[:,1], s=10.0, c=y_umap, alpha=0.75, cmap='inferno')\n",
        "plt.xlabel('First UMAP dimension')\n",
        "plt.ylabel('Second UMAP dimension')\n",
        "plt.colorbar()\n",
        "plt.savefig('Originalumap.png', bbox_inches='tight', dpi=350)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuD-MKie9s2a"
      },
      "source": [
        "# Deep Neural Network Model Experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9q5KEWHaoLw"
      },
      "source": [
        "##Set Up"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AiQUMBY4l4cR"
      },
      "source": [
        "### Set up Class Weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9BJ7lhFYNSJ"
      },
      "source": [
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Activation, Input\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing import sequence\n",
        "import numpy as np\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "inpt_dim = x_train.shape[1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umaxGtHZDI87"
      },
      "source": [
        "# Code found with https://stackoverflow.com/questions/43481490/keras-class-weights-class-weight-for-one-hot-encoding\n",
        "# Computes an initial class weight for each severity level based on its representation in the data set\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "class_weights = compute_class_weight('balanced', [1,2,3,4], y_train_values)\n",
        "d_class_weights = dict(enumerate(class_weights))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2uINTYGXmj2G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "43943b18-c916-438d-809e-b0e82623b45c"
      },
      "source": [
        "d_class_weights"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{0: 76.93535825545172,\n",
              " 1: 0.3520391435739537,\n",
              " 2: 0.9180762081784387,\n",
              " 3: 17.490262039660056}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bPWiBAugauvz"
      },
      "source": [
        "### Shared Methods \n",
        "\n",
        "Contains a method that plots the loss of a model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pI19Z7WLjUcc"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "def plotLoss(hist):\n",
        "  fig = plt.figure(figsize=(10,6))\n",
        "  plt.plot(hist.history['loss'], color='#785ef0')\n",
        "  plt.plot(hist.history['val_loss'], color='#dc267f')\n",
        "  plt.title('Model Loss Progress')\n",
        "  plt.ylabel('Categorical Cross-Entropy Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Training Set', 'Test Set'], loc='upper right')\n",
        "  plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gLkpeeWWgcIR"
      },
      "source": [
        "##Launch Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bsKoXOmHk8_q"
      },
      "source": [
        "# Load the TensorBoard notebook extension\n",
        "%reload_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Lu83D3iZk_Mn"
      },
      "source": [
        "# Clear any logs from previous runs\n",
        "! rm -rf ./logs/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vEKhVfP8kUi5"
      },
      "source": [
        "import datetime\n",
        "logdir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "J3eNboSjlFoD"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vvQn-TGjLoKY"
      },
      "source": [
        "If the tensorboard does not load, change the port value to another random value and rerun the block. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3QqptqElmCev"
      },
      "source": [
        "%tensorboard --logdir logs --host localhost --port 5001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UGvg-7nDlAIM"
      },
      "source": [
        "## Hyper-Parameter Tuning Deep Neural Network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "muBv72WPLyw6"
      },
      "source": [
        "Finds the best combination of number of units, dropout value, l2 value, and batch size value for the dense neural network\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nmxwGGXnlJ3P"
      },
      "source": [
        "HP_NUM_UNITS = hp.HParam('num_units', hp.Discrete([1108, 2*1108,3*1108]))\n",
        "HP_DROPOUT = hp.HParam('dropout', hp.Discrete([0.2, 0.4, 0.6]))\n",
        "#HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam']))\n",
        "HP_BATCH_SIZE = hp.HParam('batch_size', hp.Discrete([2000, 5000, 10000]))\n",
        "HP_L2 = hp.HParam('l2', hp.Discrete([0.001, 0.0001]))\n",
        "\n",
        "METRIC_ERROR = 'BER'\n",
        "\n",
        "with tf.summary.create_file_writer('logs/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_BATCH_SIZE, HP_L2],\n",
        "    metrics=[hp.Metric(METRIC_ERROR, display_name='BER')],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I8DvG9KaL6_r"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Softmax\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "def train_test_model(hparams):\n",
        "  deepnet = None\n",
        "\n",
        "  inpt_vec = Input(shape=(1108))\n",
        "\n",
        "  units = hparams[HP_NUM_UNITS]\n",
        "  dr = hparams[HP_DROPOUT]\n",
        "\n",
        "  dl = Dropout(dr)(inpt_vec)\n",
        "  dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(hparams[HP_L2]))(dl)\n",
        "  d1 = BatchNormalization()(dl)\n",
        "\n",
        "  units = units//2\n",
        "  dr = dr/1.5\n",
        "  while units > 4:\n",
        "    dl = Dropout(dr)(dl)\n",
        "    dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(hparams[HP_L2]))(dl)\n",
        "    d1 = BatchNormalization()(dl)\n",
        "    units = units//2\n",
        "    dr = dr/1.5\n",
        "\n",
        "  output = Dense(4, activation=tf.nn.softmax)(dl)\n",
        "\n",
        "  deepnet = Model(inpt_vec, output)\n",
        "\n",
        "  deepnet.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam')\n",
        "  deepnet.summary()\n",
        "\n",
        "  callbacks=[\n",
        "      tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1), # log metrics\n",
        "      hp.KerasCallback(logdir, hparams),  # log hparams\n",
        "      ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,  \n",
        "                                 min_delta=1e-4, mode='min'),\n",
        "      EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "  ],\n",
        "\n",
        "  hist = deepnet.fit(x_train, y_train, batch_size=hparams[HP_BATCH_SIZE], epochs=200, \n",
        "                    callbacks=callbacks, shuffle=True, verbose=0,\n",
        "                    validation_data=(x_test, y_test), class_weight=d_class_weights)\n",
        "\n",
        "  plotLoss(hist)\n",
        "  \n",
        "  y_hat = deepnet.predict(x_validate)    # we take the neuron with maximum\n",
        "  print(y_hat)\n",
        "  y_pred = np.argmax(y_hat, axis=1) + 1 # output as our prediction\n",
        "  print(y_pred)\n",
        "\n",
        "  y_true = y_validate   # this is the ground truth\n",
        "  labels=[1, 2, 3, 4]\n",
        "\n",
        "  print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "  print(cm)\n",
        "\n",
        "  ber = 1- balanced_accuracy_score(y_true, y_pred)\n",
        "  print(\"ber:\", ber)\n",
        "\n",
        "  return ber\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F6IFRuhFlnIt"
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    ber = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ERROR, ber, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jaSqxQp-mAHM"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    for l2 in HP_L2.domain.values:\n",
        "      for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "        hparams = {\n",
        "            HP_NUM_UNITS: num_units,\n",
        "            HP_DROPOUT: dropout_rate,\n",
        "            HP_L2: l2,\n",
        "            HP_BATCH_SIZE: batch_size,\n",
        "        }\n",
        "        run_name = \"deep-neural-net-run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run('logs/hparam_tuning/' + run_name, hparams)\n",
        "        session_num += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sisRokNEZDsa"
      },
      "source": [
        "## Hyper Parameter Tuning Deep Neural Network with AutoEncoder Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFHX_lTnaF0c"
      },
      "source": [
        "from tensorflow import keras\n",
        "import datetime\n",
        "import tensorflow as tf\n",
        "from tensorboard.plugins.hparams import api as hp\n",
        "latent_ncdr = keras.models.load_model('latent_ncdr.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "53w4rCW15M9K"
      },
      "source": [
        "! rm -rf ./logs/ae"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zEIzswxqnfY9"
      },
      "source": [
        "logdir = \"logs/fit/ae/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fhcx5yp6npXU"
      },
      "source": [
        "%tensorboard --logdir logs/ae/ --host localhost --port 5010"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dnwa1ELWnh-P"
      },
      "source": [
        "\n",
        "with tf.summary.create_file_writer('logs/ae/hparam_tuning').as_default():\n",
        "  hp.hparams_config(\n",
        "    hparams=[HP_NUM_UNITS, HP_DROPOUT, HP_BATCH_SIZE, HP_L2],\n",
        "    metrics=[hp.Metric(METRIC_ERROR, display_name='BER')],\n",
        ")\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zobUjyNymRkY"
      },
      "source": [
        "x_encdd_train = latent_ncdr.predict(x_train)\n",
        "x_encdd_test = latent_ncdr.predict(x_test)\n",
        "x_encdd_validate = latent_ncdr.predict(x_validate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KauSXo8wZPX3"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Softmax\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "def train_test_model(hparams):\n",
        "  deepnet = None\n",
        "\n",
        "  inpt_vec = Input(shape=(256))\n",
        "\n",
        "  units = hparams[HP_NUM_UNITS]\n",
        "  dr = hparams[HP_DROPOUT]\n",
        "\n",
        "  dl = Dropout(dr)(inpt_vec)\n",
        "  dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(hparams[HP_L2]))(dl)\n",
        "  d1 = BatchNormalization()(dl)\n",
        "\n",
        "  units = units//2\n",
        "  dr = dr/1.5\n",
        "  while units > 4:\n",
        "    dl = Dropout(dr)(dl)\n",
        "    dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(hparams[HP_L2]))(dl)\n",
        "    d1 = BatchNormalization()(dl)\n",
        "    units = units//2\n",
        "    dr = dr/1.5\n",
        "\n",
        "  output = Dense(4, activation=tf.nn.softmax)(dl)\n",
        "\n",
        "  deepnet = Model(inpt_vec, output)\n",
        "\n",
        "  deepnet.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam')\n",
        "  deepnet.summary()\n",
        "\n",
        "  callbacks=[\n",
        "      tf.keras.callbacks.TensorBoard(log_dir=logdir, histogram_freq=1), # log metrics\n",
        "      hp.KerasCallback(logdir, hparams),  # log hparams\n",
        "      ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,  \n",
        "                                 min_delta=1e-4, mode='min'),\n",
        "      EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "  ],\n",
        "\n",
        "  hist = deepnet.fit(x_encdd_train, y_train, batch_size=hparams[HP_BATCH_SIZE], epochs=200, \n",
        "                    callbacks=callbacks, shuffle=True, verbose=0,\n",
        "                    validation_data=(x_encdd_test, y_test), class_weight=d_class_weights)\n",
        "\n",
        "  plotLoss(hist)\n",
        "  \n",
        "  y_hat = deepnet.predict(x_encdd_validate)    # we take the neuron with maximum\n",
        "  print(y_hat)\n",
        "  y_pred = np.argmax(y_hat, axis=1) + 1 # output as our prediction\n",
        "  print(y_pred)\n",
        "\n",
        "  y_true = y_validate   # this is the ground truth\n",
        "  labels=[1, 2, 3, 4]\n",
        "\n",
        "  print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "  cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "  print(cm)\n",
        "\n",
        "  ber = 1- balanced_accuracy_score(y_true, y_pred)\n",
        "  print(\"ber:\", ber)\n",
        "\n",
        "  return ber"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4Ec2ZNWum1hX"
      },
      "source": [
        "def run(run_dir, hparams):\n",
        "  with tf.summary.create_file_writer(run_dir).as_default():\n",
        "    hp.hparams(hparams)  # record the values used in this trial\n",
        "    ber = train_test_model(hparams)\n",
        "    tf.summary.scalar(METRIC_ERROR, ber, step=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1m1wr6Twm1hY"
      },
      "source": [
        "session_num = 0\n",
        "\n",
        "for num_units in HP_NUM_UNITS.domain.values:\n",
        "  for dropout_rate in HP_DROPOUT.domain.values:\n",
        "    for l2 in HP_L2.domain.values:\n",
        "      for batch_size in HP_BATCH_SIZE.domain.values:\n",
        "        hparams = {\n",
        "            HP_NUM_UNITS: num_units,\n",
        "            HP_DROPOUT: dropout_rate,\n",
        "            HP_L2: l2,\n",
        "            HP_BATCH_SIZE: batch_size,\n",
        "        }\n",
        "        run_name = \"AE-deep-neural-net-run-%d\" % session_num\n",
        "        print('--- Starting trial: %s' % run_name)\n",
        "        print({h.name: hparams[h] for h in hparams})\n",
        "        run('logs/ae/hparam_tuning/' + run_name, hparams)\n",
        "        session_num += 1\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v5hOf_fHmESc"
      },
      "source": [
        "## Deep Neural Network with AutoEncoder Input"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CVSsATmfmESd"
      },
      "source": [
        "from tensorflow import keras\n",
        "latent_ncdr = keras.models.load_model('latent_ncdr.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VyiPxDdbmESe"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Softmax\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "num_units= 1108\n",
        "dropout = 0.2\n",
        "batch_size = 10000\n",
        "\n",
        "deepnet = None\n",
        "\n",
        "x_encdd_train = latent_ncdr.predict(x_train)\n",
        "x_encdd_test = latent_ncdr.predict(x_test)\n",
        "x_encdd_validate = latent_ncdr.predict(x_validate)\n",
        "\n",
        "inpt_vec = Input(shape=(256))\n",
        "\n",
        "units = num_units\n",
        "dr =dropout\n",
        "\n",
        "dl = Dropout(dr)(inpt_vec)\n",
        "dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l1(0.0001))(dl)\n",
        "d1 = BatchNormalization()(dl)\n",
        "\n",
        "units = units//2\n",
        "dr = dr/1.5\n",
        "while units > 4:\n",
        "  dl = Dropout(dr)(dl)\n",
        "  dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l1(0.0001))(dl)\n",
        "  d1 = BatchNormalization()(dl)\n",
        "  units = units//2\n",
        "  dr = dr/1.5\n",
        "\n",
        "output = Dense(4, activation=tf.nn.softmax)(dl)\n",
        "\n",
        "deepnet = Model(inpt_vec, output)\n",
        "\n",
        "deepnet.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam')\n",
        "deepnet.summary()\n",
        "\n",
        "callbacks=[\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,  \n",
        "                                 min_delta=1e-4, mode='min'),\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "],\n",
        "\n",
        "hist = deepnet.fit(x_encdd_train, y_train, batch_size=batch_size, epochs=200, \n",
        "                    callbacks=callbacks, shuffle=True, verbose=0,\n",
        "                    validation_data=(x_encdd_test, y_test), class_weight=d_class_weights)\n",
        "\n",
        "plotLoss(hist)\n",
        "  \n",
        "y_hat = deepnet.predict(x_encdd_validate)    # we take the neuron with maximum\n",
        "print(y_hat)\n",
        "y_pred = np.argmax(y_hat, axis=1) + 1 # output as our prediction\n",
        "print(y_pred)\n",
        "\n",
        "print(np.unique(y_pred))\n",
        "\n",
        "y_true = y_validate   # this is the ground truth\n",
        "labels=[1, 2, 3, 4]\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "print(cm)\n",
        "\n",
        "ber = 1- balanced_accuracy_score(y_true, y_pred)\n",
        "print(\"ber:\", ber)\n",
        "\n",
        "deepnet.save(\"deepnetWithAutoEncoderData.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "75T5YSsuUGvi"
      },
      "source": [
        "## Final Model - no class weights"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LBndxSnJUIVL"
      },
      "source": [
        "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Softmax\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import balanced_accuracy_score\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.python.keras import regularizers\n",
        "\n",
        "num_units= 1108\n",
        "dropout = 0.2\n",
        "batch_size = 5000\n",
        "\n",
        "deepnet = None\n",
        "\n",
        "inpt_vec = Input(shape=(1108))\n",
        "\n",
        "units = num_units\n",
        "dr =dropout\n",
        "\n",
        "dl = Dropout(dr)(inpt_vec)\n",
        "dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(dl)\n",
        "d1 = BatchNormalization()(dl)\n",
        "\n",
        "units = units//2\n",
        "dr = dr/1.5\n",
        "while units > 4:\n",
        "  dl = Dropout(dr)(dl)\n",
        "  dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(dl)\n",
        "  d1 = BatchNormalization()(dl)\n",
        "  units = units//2\n",
        "  dr = dr/1.5\n",
        "\n",
        "output = Dense(4, activation=tf.nn.softmax)(dl)\n",
        "\n",
        "deepnet = Model(inpt_vec, output)\n",
        "\n",
        "deepnet.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam')\n",
        "deepnet.summary()\n",
        "\n",
        "callbacks=[\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,  \n",
        "                                 min_delta=1e-4, mode='min'),\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "],\n",
        "\n",
        "hist = deepnet.fit(x_train, y_train, batch_size=batch_size, epochs=200, \n",
        "                    callbacks=callbacks, shuffle=True, verbose=0,\n",
        "                    validation_data=(x_test, y_test))\n",
        "\n",
        "plotLoss(hist)\n",
        "  \n",
        "y_hat = deepnet.predict(x_validate)    # we take the neuron with maximum\n",
        "print(y_hat)\n",
        "y_pred = np.argmax(y_hat, axis=1) + 1 # output as our prediction\n",
        "print(y_pred)\n",
        "\n",
        "print(np.unique(y_pred))\n",
        "\n",
        "y_true = y_validate   # this is the ground truth\n",
        "print(y_validate)\n",
        "print(y_true)\n",
        "labels=[1, 2, 3, 4]\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "print(cm)\n",
        "\n",
        "ber = 1- balanced_accuracy_score(y_true, y_pred)\n",
        "print(\"ber:\", ber)\n",
        "\n",
        "deepnet.save(\"finalModelNoClassWeights.hdf5\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HPUUW4pzZQUX"
      },
      "source": [
        "# Final Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fWzBo4LlZQUX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 229
        },
        "outputId": "b5de2be1-c8df-48e5-89e1-235592da8329"
      },
      "source": [
        "num_units= 1108\n",
        "dropout = 0.2\n",
        "batch_size = 5000\n",
        "\n",
        "deepnet = None\n",
        "\n",
        "inpt_vec = Input(shape=(1108))\n",
        "\n",
        "units = num_units\n",
        "dr =dropout\n",
        "\n",
        "dl = Dropout(dr)(inpt_vec)\n",
        "dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(dl)\n",
        "d1 = BatchNormalization()(dl)\n",
        "\n",
        "units = units//2\n",
        "dr = dr/1.5\n",
        "while units > 4:\n",
        "  dl = Dropout(dr)(dl)\n",
        "  dl = Dense(units, activation='relu', kernel_regularizer=regularizers.l2(0.0001))(dl)\n",
        "  d1 = BatchNormalization()(dl)\n",
        "  units = units//2\n",
        "  dr = dr/1.5\n",
        "\n",
        "output = Dense(4, activation=tf.nn.softmax)(dl)\n",
        "\n",
        "deepnet = Model(inpt_vec, output)\n",
        "\n",
        "deepnet.compile(loss=tf.keras.losses.categorical_crossentropy, optimizer='adam')\n",
        "deepnet.summary()\n",
        "\n",
        "callbacks=[\n",
        "    ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=20,  \n",
        "                                 min_delta=1e-4, mode='min'),\n",
        "    EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
        "],\n",
        "\n",
        "hist = deepnet.fit(x_train, y_train, batch_size=batch_size, epochs=200, \n",
        "                    callbacks=callbacks, shuffle=True, verbose=0,\n",
        "                    validation_data=(x_test, y_test), class_weight=d_class_weights)\n",
        "\n",
        "plotLoss(hist)\n",
        "  \n",
        "y_hat = deepnet.predict(x_validate)    # we take the neuron with maximum\n",
        "print(y_hat)\n",
        "y_pred = np.argmax(y_hat, axis=1) + 1 # output as our prediction\n",
        "print(y_pred)\n",
        "\n",
        "y_true = y_validate   # this is the ground truth\n",
        "labels=[1, 2, 3, 4]\n",
        "\n",
        "print(classification_report(y_true, y_pred, labels=labels))\n",
        "\n",
        "cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
        "print(cm)\n",
        "\n",
        "ber = 1- balanced_accuracy_score(y_true, y_pred)\n",
        "print(\"ber:\", ber)\n",
        "\n",
        "deepnet.save(\"FinalModel.hdf5\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-23-b11093ab82fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minpt_vec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0mdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0munits\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mactivation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkernel_regularizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mregularizers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ml2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.0001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0md1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'regularizers' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PDAWmqcbsCKO"
      },
      "source": [
        "## "
      ]
    }
  ]
}